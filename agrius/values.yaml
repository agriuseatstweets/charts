tags:
  dashboard: false
  kafka: true

belly-claws:
  image:
    repository: agriuseatstweets/bellygo
    tag: 0.0.4
  schedule: "0 */2 * * *"
  completions: 1
  parallelism: 1
  backoffLimit: 5
  resources:
    requests:
      cpu: "1000m"
      memory: "1Gi"
  keySecret: agrius-belly-keys
  env:
    - name: BELLY_TOPIC
      value: "tweets"
    - name: BELLY_LOCATION
      value: "spain-tweets-datalake"
    - name: BELLY_SIZE
      value: "360000"
    - name: BELLY_GROUP
      value: "belly"
    - name: KAFKA_BROKERS
      value: "kafka-headless:29092"
    - name: KAFKA_POLL_TIMEOUT
      value: "600s"


belly-jaws:
  image:
    repository: agriuseatstweets/bellygo
    tag: 0.0.4
  schedule: "10 0 * * *"
  completions: 1
  parallelism: 1
  backoffLimit: 1
  resources:
    requests:
      cpu: "1000m"
      memory: "1Gi"
  keySecret: agrius-belly-keys
  env:
    - name: BELLY_TOPIC
      value: "livetweets"
    - name: BELLY_LOCATION
      value: "spain-tweets-datalake"
    - name: BELLY_SIZE
      value: "360000"
    - name: BELLY_GROUP
      value: "belly"
    - name: KAFKA_BROKERS
      value: "kafka-headless:29092"
    - name: KAFKA_POLL_TIMEOUT
      value: "600s"

claws:
  image:
    repository: agriuseatstweets/claws
    tag: 0.0.9
  schedule: "10 0 * * *"
  envSecret: agrius-jaws-envs
  keySecret: agrius-jaws-keys
  activeDeadlineSeconds: 86040
  queue:
    queue: "kafka"
    kafkaBrokers: "kafka-headless:29092"
    topic: "tweets"
    digTopic: "dig"
  sheets:
    sheetId: "1WVqs8BBFhgP-u2WsQEip0XxJkYDWcucra2iHPbs52Lc"
    users: "follows!D2:D"
    hashtags: "follows!C2:C"
    urls: "follows!B2:B"
    locations: "follows!F2:F"
  resources:
    requests:
      cpu: "100m"
      memory: "128Mi"

jaws:
  image:
    repository: agriuseatstweets/jaws
    tag: 0.0.10
  envSecret: agrius-jaws-envs
  keySecret: agrius-jaws-keys
  refreshInterval: "30"
  queue:
    queue: "kafka"
    kafkaBrokers: "kafka-headless:29092"
    topic: "livetweets"
  sheets:
    sheetId: "1WVqs8BBFhgP-u2WsQEip0XxJkYDWcucra2iHPbs52Lc"
    users: "follows!D2:D"
    hashtags: "follows!C2:C"
    urls: "follows!B2:B"
    locations: "follows!E2:E"
  java:
    toolOptions: "-Xms256m -Xmx4096m -XX:MaxGCPauseMillis=1000"
  resources:
    requests:
      cpu: "100m"
      memory: "512Mi"

kafka-exporter:
  kafkaExporter:
    kafka:
      servers:
        - kafka-headless:29092



# kafka:
#   replicas: 3
#   imageTag: 5.4.0
#   resources:
#     requests:
#       cpu: 100m
#       memory: 4Gi
#   kafkaHeapOptions: "-Xmx4G -Xms1G"
#   configurationOverrides:
#     "offsets.topic.replication.factor": 3
#     "group.initial.rebalance.delay.ms": 20000
#     "auto.create.topics.enable": false
#     "auto.leader.rebalance.enable": false
#   topics:
#     - name: tweets
#       config: "retention.ms=1728000000,compression.type=snappy" # 20 days
#       partitions: 8
#       replicationFactor: 3
#     - name: livetweets
#       config: "retention.ms=1728000000,compression.type=snappy" # 20 days
#       partitions: 1
#       replicationFactor: 3
#   persistence:
#     size: "500Gi"
#     storageClass: "pd-ssd"
#   affinity:
#     podAntiAffinity:
#       requiredDuringSchedulingIgnoredDuringExecution:
#       - labelSelector:
#           matchExpressions:
#           - key: app
#             operator: In
#             values:
#             - kafka
#         topologyKey: "kubernetes.io/hostname"
#     podAffinity:
#       preferredDuringSchedulingIgnoredDuringExecution:
#        - weight: 50
#          podAffinityTerm:
#            labelSelector:
#              matchExpressions:
#              - key: app
#                operator: In
#                values:
#                  - zookeeper
#            topologyKey: "kubernetes.io/hostname"

#   prometheus:
#     ## Prometheus JMX Exporter: exposes the majority of Kafkas metrics
#     jmx:
#       enabled: true
#       ## Port jmx-exporter exposes Prometheus format metrics to scrape
#       port: 5556

#     kafka:
#       enabled: true
#       ## Port kafka-exporter exposes for Prometheus to scrape metrics
#       port: 9308

#     operator:
#       ## Are you using Prometheus Operator?
#       enabled: true
#       serviceMonitor:
#         namespace: monitoring
#         selector:
#           heritage: Helm
#           release: prometheus

#       prometheusRule:
#         ## Add Prometheus Rules?
#         enabled: true
#         ## Namespace in which to install the PrometheusRule resource.
#         namespace: monitoring
#         # Use release namespace instead
#         releaseNamespace: false

#         ## Defaults to whats used if you follow CoreOS [Prometheus Install Instructions](https://github.com/coreos/prometheus-operator/tree/master/helm#tldr)
#         ## [Prometheus Selector Label](https://github.com/coreos/prometheus-operator/blob/master/helm/prometheus/templates/prometheus.yaml#L65)
#         ## [Kube Prometheus Selector Label](https://github.com/coreos/prometheus-operator/blob/master/helm/kube-prometheus/values.yaml#L298)
#         selector:
#           heritage: Helm
#           release: prometheus
#           prometheus: null

#         ## Some example rules.
#         ## e.g. max(kafka_controller_kafkacontroller_activecontrollercount_value{service="my-kafka-release"}) by (service) < 1
#         rules:
#         - alert: KafkaNoActiveControllers
#           annotations:
#             message: The number of active controllers in {{ "{{" }} $labels.namespace {{ "}}" }} is less than 1. This usually means that some of the Kafka nodes aren't communicating properly. If it doesn't resolve itself you can try killing the pods (one by one whilst monitoring the under-replicated partitions graph).
#           expr: max(kafka_controller_kafkacontroller_activecontrollercount_value) by (namespace) < 1
#           for: 5m
#           labels:
#             severity: critical
#         - alert: KafkaMultipleActiveControllers
#           annotations:
#             message: The number of active controllers in {{ "{{" }} $labels.namespace {{ "}}" }} is greater than 1. This usually means that some of the Kafka nodes aren't communicating properly. If it doesn't resolve itself you can try killing the pods (one by one whilst monitoring the under-replicated partitions graph).
#           expr: max(kafka_controller_kafkacontroller_activecontrollercount_value) by (namespace) > 1
#           for: 5m
#           labels:
#             severity: critical
